{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching in Ragas\n",
    "\n",
    "Ragas offers a flexible caching system to speed up evaluations and testset generation by avoiding redundant computations, especially for LLM and embedding model calls. This document explains how to use both exact (disk-based) caching and the more advanced semantic caching.\n",
    "\n",
    "## Global Cache Configuration\n",
    "\n",
    "Ragas now features a global cache configuration managed by `ragas.config.ragas_cache`. This cache is initialized based on environment variables when Ragas is imported. You can control caching behavior by setting these variables:\n",
    "\n",
    "*   **`RAGAS_CACHE_ENABLED`**: (boolean, default: \"true\") - Set to \"false\" to disable all caching.\n",
    "*   **`RAGAS_CACHE_BACKEND`**: (string, default: \"exact\") - Choose between `\"exact\"` (for `DiskCacheBackend`) or `\"semantic\"` (for `SemanticCacheBackend`).\n",
    "*   **`RAGAS_CACHE_DIR`**: (string, default: \".ragas_cache\") - Directory for `DiskCacheBackend`.\n",
    "*   **`RAGAS_SEMANTIC_CACHE_THRESHOLD`**: (float, default: 0.85) - Cosine similarity threshold for `SemanticCacheBackend`.\n",
    "*   **`RAGAS_SEMANTIC_CACHE_EMBEDDING_PROVIDER`**: (string, default: \"openai\") - Embedding provider (e.g., \"openai\", \"huggingface\") for `SemanticCacheBackend`.\n",
    "*   **`RAGAS_SEMANTIC_CACHE_EMBEDDING_MODEL_NAME`**: (string, optional) - Specific model name for the embedding provider. Defaults to standard models like \"text-embedding-ada-002\" for OpenAI.\n",
    "\n",
    "Functions decorated with `@cacher()` (like those within LLM and embedding wrappers) will automatically use this globally configured cache.\n",
    "\n",
    "You can also implement your own custom cacher by implementing the [CacheInterface][ragas.cache.CacheInterface]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Exact (Disk) Caching\n",
    "\n",
    "Exact caching stores results based on an exact match of the input arguments. This is typically handled by the `DiskCacheBackend`.\n",
    "\n",
    "**Default Behavior (Environment Controlled):**\n",
    "If you haven't changed `RAGAS_CACHE_BACKEND` (or it's explicitly set to `\"exact\"`), Ragas will use `DiskCacheBackend` by default. You can control the cache directory with `RAGAS_CACHE_DIR`.\n",
    "\n",
    "**Explicit Usage:**\n",
    "You can also instantiate `DiskCacheBackend` directly and pass it to specific components if you need finer-grained control or multiple cache instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.cache import DiskCacheBackend\n",
    "\n",
    "# Example of explicit instantiation\n",
    "disk_cache = DiskCacheBackend(cache_dir=\".my_custom_cache_location\")\n",
    "\n",
    "# This instance can then be passed to LLM/embedding wrappers if needed:\n",
    "# from ragas.llms import LangchainLLMWrapper\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# cached_llm = LangchainLLMWrapper(ChatOpenAI(), cache=disk_cache)\n",
    "\n",
    "print(f\"DiskCacheBackend initialized at: {disk_cache.cache_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Example with Exact Caching\n",
    "\n",
    "Let's set up an evaluation. If `RAGAS_CACHE_ENABLED` is \"true\" and `RAGAS_CACHE_BACKEND` is \"exact\" (or not set), the LLM calls during evaluation will be cached to disk.\n",
    "\n",
    "First, ensure your environment variables are set (or rely on defaults for exact caching). For this example, we'll simulate the default behavior by explicitly creating a `LangchainLLMWrapper` that would use the global cache. If you were running this in a fresh environment with default settings, the `@cacher()` inside `LangchainLLMWrapper`'s methods would pick up the global `DiskCacheBackend`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure RAGAS_CACHE_ENABLED=true and RAGAS_CACHE_BACKEND=exact (or defaults)\n",
    "# For demonstration, we'll explicitly pass a DiskCacheBackend instance\n",
    "# In a typical scenario with env vars set, this explicit passing isn't needed for default behavior.\n",
    "\n",
    "import os\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Simulate global cache being a DiskCacheBackend\n",
    "# If RAGAS_CACHE_ENABLED is true and RAGAS_CACHE_BACKEND is 'exact' or unset,\n",
    "# ragas.config.ragas_cache would be a DiskCacheBackend instance.\n",
    "# We create one here for clarity in the example.\n",
    "llm_cache_instance = DiskCacheBackend(cache_dir=\".ragas_cache/llm_calls\")\n",
    "\n",
    "cached_llm = LangchainLLMWrapper(\n",
    "    ChatOpenAI(model=\"gpt-3.5-turbo\"), # Using a faster model for example\n",
    "    cache=llm_cache_instance # Explicitly passing cache for this example\n",
    ")\n",
    "\n",
    "# To see cache in action, set logging (optional)\n",
    "import logging\n",
    "from ragas.utils import set_logging_level\n",
    "set_logging_level(\"ragas.cache\", logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness\n",
    "from datasets import Dataset\n",
    "\n",
    "# Prepare a very small dataset for quick example\n",
    "dummy_data = {\n",
    "    'question': ['What is the capital of France?'],\n",
    "    'answer': ['The capital of France is Paris.'],\n",
    "    'contexts': [['France is a country in Western Europe. Paris is its capital.']],\n",
    "    'ground_truth': ['Paris is the capital of France.']\n",
    "}\n",
    "eval_dataset = Dataset.from_dict(dummy_data)\n",
    "\n",
    "print(\"Running evaluation (first pass)...\")\n",
    "results_pass1 = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=[faithfulness],\n",
    "    llm=cached_llm\n",
    ")\n",
    "print(results_pass1)\n",
    "\n",
    "print(\"\\nRunning evaluation (second pass, should use cache)...\")\n",
    "results_pass2 = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=[faithfulness],\n",
    "    llm=cached_llm\n",
    ")\n",
    "print(results_pass2)\n",
    "\n",
    "assert results_pass1.to_pandas().equals(results_pass2.to_pandas()), \"Results should be identical\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pass will make LLM calls and store them. The second pass should be significantly faster as it retrieves results from the cache. You'll see DEBUG logs from `ragas.cache` if logging is enabled, indicating cache hits.\n",
    "\n",
    "This caching also applies to testset generation if the `generator_llm` or other components are wrapped with the `@cacher()` and use the global cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Caching\n",
    "\n",
    "Semantic caching offers a more intelligent way to cache results, especially for LLM-generated content. Instead of relying on exact input matches, it caches based on the semantic similarity of a designated part of the input (e.g., a user's query or a document), while still requiring exact matches for other parameters (like function name or specific model settings).\n",
    "\n",
    "**Benefits:**\n",
    "*   **Resilience to Paraphrasing:** If a query is slightly rephrased but retains the same meaning, semantic cache can still provide a hit.\n",
    "*   **Reduced LLM Calls:** Can significantly cut down on LLM API usage even if inputs aren't identical byte-for-byte.\n",
    "\n",
    "The `SemanticCacheBackend` handles this. It works by:\n",
    "1.  Parsing the structured key (function name, args, kwargs).\n",
    "2.  Identifying a primary string argument for semantic comparison.\n",
    "3.  Embedding this string argument.\n",
    "4.  Comparing this embedding with stored embeddings using cosine similarity.\n",
    "5.  If similarity is above a threshold, it then checks for exact matches on all other non-semantic parts of the key.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Semantic Caching via Environment Variables\n",
    "\n",
    "This is the easiest way to enable semantic caching globally for your Ragas application. Set these environment variables **before importing Ragas**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example environment variable setup (run this in your terminal or a setup script)\n",
    "# For a Python notebook, you might need to set these before the kernel starts, \n",
    "# or use os.environ and then re-import/reload Ragas modules if already imported.\n",
    "\n",
    "# os.environ[\"RAGAS_CACHE_ENABLED\"] = \"true\" # Default, but good to be explicit\n",
    "# os.environ[\"RAGAS_CACHE_BACKEND\"] = \"semantic\"\n",
    "# os.environ[\"RAGAS_SEMANTIC_CACHE_EMBEDDING_PROVIDER\"] = \"openai\" \n",
    "# os.environ[\"RAGAS_SEMANTIC_CACHE_EMBEDDING_MODEL_NAME\"] = \"text-embedding-ada-002\"\n",
    "# os.environ[\"RAGAS_SEMANTIC_CACHE_THRESHOLD\"] = \"0.85\" # Example threshold\n",
    "\n",
    "# After setting these, if you run an evaluation, LangchainLLMWrapper (if used)\n",
    "# will automatically use SemanticCacheBackend via ragas.config.ragas_cache.\n",
    "\n",
    "print(\"Imagine environment variables are set as above.\")\n",
    "print(\"If Ragas is imported now, ragas.config.ragas_cache would be a SemanticCacheBackend.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you had set the environment variables as shown above, running the previous evaluation example (without explicitly passing a `cache` object to `LangchainLLMWrapper`) would automatically use semantic caching for the LLM calls. The `@cacher()` decorator within the wrapper would pick up the globally configured `SemanticCacheBackend`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `SemanticCacheBackend` Directly\n",
    "\n",
    "For more control, you can instantiate `SemanticCacheBackend` and pass it to components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.cache import SemanticCacheBackend\n",
    "from ragas.embeddings import OpenAIEmbeddings # Make sure you have 'openai' extras installed\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 1. Initialize your embedding model\n",
    "# Ensure you have OPENAI_API_KEY set in your environment for this to work\n",
    "try:\n",
    "    my_embed_model = OpenAIEmbeddings(model_name=\"text-embedding-ada-002\")\n",
    "except ImportError:\n",
    "    print(\"OpenAI embeddings not available. Install with 'pip install ragas[openai]'\")\n",
    "    my_embed_model = None\n",
    "\n",
    "if my_embed_model:\n",
    "    # 2. Create the SemanticCacheBackend instance\n",
    "    semantic_cache = SemanticCacheBackend(\n",
    "        embedding_model=my_embed_model, \n",
    "        similarity_threshold=0.85 # Adjust as needed\n",
    "    )\n",
    "\n",
    "    # 3. Use this cache with an LLM wrapper\n",
    "    semantic_cached_llm = LangchainLLMWrapper(\n",
    "        ChatOpenAI(model=\"gpt-3.5-turbo\"), \n",
    "        cache=semantic_cache\n",
    "    )\n",
    "\n",
    "    print(\"SemanticCacheBackend initialized and LLM wrapped.\")\n",
    "\n",
    "    # Example of how it might be used (conceptual)\n",
    "    # Note: The LangchainLLMWrapper's methods are decorated with @cacher().\n",
    "    # When you call generate_text or generate_text_with_image, \n",
    "    # the cacher will use the 'semantic_cache' instance we passed.\n",
    "\n",
    "    # First call (original query)\n",
    "    # response1 = semantic_cached_llm.generate_text(prompt=\"Tell me about the Eiffel Tower.\")\n",
    "    # print(f\"Response 1: {response1.generations[0][0].text[:50]}...\")\n",
    "\n",
    "    # Second call (semantically similar query)\n",
    "    # response2 = semantic_cached_llm.generate_text(prompt=\"What do you know regarding the Eiffel Tower?\")\n",
    "    # print(f\"Response 2: {response2.generations[0][0].text[:50]}...\")\n",
    "    \n",
    "    # If semantic caching worked and the prompts were deemed similar enough by the embedding model\n",
    "    # and the threshold, the second call would be a cache hit (assuming other parameters like model settings are identical).\n",
    "else:\n",
    "    print(\"Skipping direct SemanticCacheBackend example as embedding model failed to load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Considerations for Semantic Caching:**\n",
    "*   **Embedding Model Choice:** The quality of your semantic cache heavily depends on the chosen embedding model. Models good at capturing nuanced semantic similarity for your domain will perform best.\n",
    "*   **Threshold Tuning:** The `similarity_threshold` is critical. Too low, and you might get false positives (caching unrelated content). Too high, and you'll miss many potential cache hits. This often requires experimentation.\n",
    "*   **Cost of Embedding:** While caching saves on LLM calls, generating embeddings itself has a small cost. For very short, frequently changing semantic parts, this might be a factor, though usually negligible compared to LLM costs.\n",
    "*   **Key Structure:** `SemanticCacheBackend` expects the cached function's arguments to be serializable into a JSON key, with the first string argument in `args` being the candidate for semantic comparison. This is handled by the `@cacher` decorator when applied to methods like those in `LangchainLLMWrapper`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
